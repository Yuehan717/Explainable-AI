# Paper Reading: “Why Should I Trust You?” Explaining the Predictions of Any Classifier
- [Introduction](#introduction)
- [Main idea](#main-idea)
- [LIME](#lime)
  - [Sparse Linear Explanations](#sparse-linear-explanations)
- [SP-LIME](#sp-lime)
- [Example](#example)
## Introduction
As machine learning classifiers are begining to aid humans in their decisions, it is important for human to understand the models being used. People tend to give more trust to explainable models and more willing to use them. Also the explainations can help avoid wrong trust, which may result in some catastrophizes. Based on this concern, the paper first give a novel explaination technique that explains the predictions of __any__ classifier, then also have a try to explain the whole model. The contributions are stated as follows.<br>
* LIME, an algorithm that can explain the predictions of _any_ classifier or regressor in a faithful way.<br>
* SP-LIME, a method to address "explain the model" problem.<br>
* Comprehensive evaluation with simulated and human subjects, where they measure the impact of explanations on trust and associated tasks.<br>
## Main idea
  In general, the idea of explaining single prediction is finding a model g to simulate the behavior of the classifier around the prediction that being explained. Then the idea of explaning the model is selecting a set of representative predictions and using their explanations as the explanation of the model. I think for predictions explanation, the key problem is follows.
  * What kind of model g should we choose?<br>
  The model g should be explainable, which means being understandable even for a laymen. (This makes a difference from "interpretable".)<br>
  * How to generate the model g?<br>
  Model g should be faithful to original classifier around the prediction and also can't have high complexity.<br>
__For model explanations, also two key problems.__
  * How many instances that humans are willing to see to build their trust in model?
  * What is the rule of selecting representative instances?
## LIME
_The overall goal of LIME
Let me show the explanation produced by LIME first.<br>
![](https://github.com/Yuehan717/Explainable-AI/blob/master/Why%20should%20I%20trust%20you%3F/function1.jpg)<br>

### Sparse Linear Explanations

## SP-LIME

## Example
